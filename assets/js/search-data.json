{
  
    
        "post0": {
            "title": "Simply Stepwise and Weighted Regression",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Ridge from sklearn.linear_model import Lasso import statsmodels.api as sm import pylab as py from sklearn import linear_model from sklearn.linear_model import LinearRegression import statsmodels.api as sm . url = &quot;Car/toyota.csv&quot; data = pd.read_csv(url) data.head() . model year price transmission mileage fuelType tax mpg engineSize . 0 GT86 | 2016 | 16000 | Manual | 24089 | Petrol | 265 | 36.2 | 2.0 | . 1 GT86 | 2017 | 15995 | Manual | 18615 | Petrol | 145 | 36.2 | 2.0 | . 2 GT86 | 2015 | 13998 | Manual | 27469 | Petrol | 265 | 36.2 | 2.0 | . 3 GT86 | 2017 | 18998 | Manual | 14736 | Petrol | 150 | 36.2 | 2.0 | . 4 GT86 | 2017 | 17498 | Manual | 36284 | Petrol | 145 | 36.2 | 2.0 | . data.shape . (6738, 10) . Stepwise rewgression . num = [&#39;mileage&#39;, &#39;tax&#39;, &#39;mpg&#39;, &#39;engineSize&#39;] # num = [&#39;mileage&#39;, &#39;tax&#39;, &#39;mpg&#39;, &#39;engineSize&#39;, &#39;engineSize2&#39;] . num = [&#39;mileage&#39;, &#39;tax&#39;, &#39;mpg&#39;, &#39;engineSize&#39;] for i in range(0,len(num),1): x = pd.DataFrame(data[num[0:i+1]]) y = data[&#39;price&#39;] model = sm.OLS(y, x).fit() print(&quot;With predictor {}&quot;.format(num[0:i+1])) print(&quot;R-squared = &quot;, model.rsquared) print(&quot;Adj R-squared =&quot;, model.rsquared_adj) print(&quot;AIC =&quot;, model.aic) print(&quot;BIC =&quot;, model.bic) print(&quot; n&quot;) . With predictor [&#39;mileage&#39;] R-squared = 0.3573100248074934 Adj R-squared = 0.3572146277501693 AIC = 144834.10437945105 BIC = 144840.91989787502 With predictor [&#39;mileage&#39;, &#39;tax&#39;] R-squared = 0.6362397640051107 Adj R-squared = 0.6361317591844472 AIC = 141001.0537912962 BIC = 141014.68482814415 With predictor [&#39;mileage&#39;, &#39;tax&#39;, &#39;mpg&#39;] R-squared = 0.7965603667829384 Adj R-squared = 0.7964697477926412 AIC = 137087.42922847485 BIC = 137107.87578374674 With predictor [&#39;mileage&#39;, &#39;tax&#39;, &#39;mpg&#39;, &#39;engineSize&#39;] R-squared = 0.9423743984966928 Adj R-squared = 0.9423401688551702 AIC = 128590.10000072751 BIC = 128617.36207442338 . model.summary() . OLS Regression Results Dep. Variable: price | R-squared (uncentered): 0.942 | . Model: OLS | Adj. R-squared (uncentered): 0.942 | . Method: Least Squares | F-statistic: 2.753e+04 | . Date: Fri, 15 Apr 2022 | Prob (F-statistic): 0.00 | . Time: 11:16:33 | Log-Likelihood: -64291. | . No. Observations: 6738 | AIC: 1.286e+05 | . Df Residuals: 6734 | BIC: 1.286e+05 | . Df Model: 4 | | . Covariance Type: nonrobust | | . | coef std err t P&gt;|t| [0.025 0.975] . mileage -0.1522 | 0.002 | -68.454 | 0.000 | -0.157 | -0.148 | . tax -1.6673 | 0.562 | -2.967 | 0.003 | -2.769 | -0.566 | . mpg -1.7408 | 1.664 | -1.046 | 0.295 | -5.002 | 1.520 | . engineSize 1.112e+04 | 85.197 | 130.535 | 0.000 | 1.1e+04 | 1.13e+04 | . Omnibus: 2229.863 | Durbin-Watson: 0.914 | . Prob(Omnibus): 0.000 | Jarque-Bera (JB): 11951.306 | . Skew: 1.492 | Prob(JB): 0.00 | . Kurtosis: 8.802 | Cond. No. 6.18e+04 | . Notes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.[3] The condition number is large, 6.18e+04. This might indicate that there arestrong multicollinearity or other numerical problems. Use &quot;RegscorePy&quot; . y_pred = model.predict(x) . from RegscorePy import * . formula https://online.stat.psu.edu/stat462/node/199/ . AIC . p = 4 y = y aic.aic(y, y_pred, p) # using RegscorePy . 109256.10676454737 . n=6738 P=4 n*np.log(model.ssr) - n*np.log(n) + 2*P # using formula . 109256.10676454735 . BIC . p = 4 y = y bic.bic(y, y_pred, p) # using RegscorePy . 109283.36883824323 . n=6738 P=4 n*np.log(model.ssr) - n*np.log(n) + P*np.log(n) # using formula . 109283.36883824322 . Weighted Regression . data[num] . mileage tax mpg engineSize engineSize2 . 0 24089 | 265 | 36.2 | 2.0 | 4.0 | . 1 18615 | 145 | 36.2 | 2.0 | 4.0 | . 2 27469 | 265 | 36.2 | 2.0 | 4.0 | . 3 14736 | 150 | 36.2 | 2.0 | 4.0 | . 4 36284 | 145 | 36.2 | 2.0 | 4.0 | . ... ... | ... | ... | ... | ... | . 6733 30000 | 20 | 58.9 | 1.0 | 3.0 | . 6734 36154 | 125 | 50.4 | 1.3 | 3.3 | . 6735 46000 | 125 | 57.6 | 1.4 | 3.4 | . 6736 60700 | 125 | 50.4 | 1.3 | 3.3 | . 6737 45128 | 125 | 50.4 | 1.3 | 3.3 | . 6738 rows × 5 columns . data.mpg.min() . 2.8 . data[num].sort_values(&#39;mpg&#39;).head(50) . mileage tax mpg engineSize engineSize2 . 6562 50 | 260 | 2.8 | 2.4 | 4.4 | . 6552 3350 | 260 | 2.8 | 2.4 | 4.4 | . 6594 1259 | 265 | 2.8 | 2.4 | 4.4 | . 6595 10250 | 260 | 2.8 | 2.4 | 4.4 | . 6617 150 | 260 | 2.8 | 2.4 | 4.4 | . 6575 7123 | 260 | 2.8 | 2.4 | 4.4 | . 6596 5000 | 260 | 2.8 | 2.4 | 4.4 | . 6563 100 | 260 | 2.8 | 2.4 | 4.4 | . 6583 4 | 265 | 2.8 | 2.4 | 4.4 | . 6576 5190 | 260 | 2.8 | 2.4 | 4.4 | . 5966 16429 | 145 | 6.0 | 1.2 | 3.2 | . 6694 100000 | 265 | 23.9 | 4.2 | 6.2 | . 6693 143000 | 325 | 27.2 | 3.0 | 5.0 | . 6698 60000 | 540 | 29.7 | 4.5 | 6.5 | . 6667 6254 | 145 | 30.1 | 2.8 | 4.8 | . 6680 789 | 145 | 30.1 | 2.8 | 4.8 | . 6677 22845 | 150 | 30.1 | 2.8 | 4.8 | . 6649 11712 | 145 | 30.1 | 2.8 | 4.8 | . 6657 10083 | 145 | 30.1 | 2.8 | 4.8 | . 6658 11619 | 145 | 30.1 | 2.8 | 4.8 | . 6682 3500 | 145 | 30.1 | 2.8 | 4.8 | . 6684 4 | 145 | 30.1 | 2.8 | 4.8 | . 6687 27 | 145 | 30.1 | 2.8 | 4.8 | . 6688 1244 | 145 | 30.1 | 2.8 | 4.8 | . 6689 4512 | 145 | 30.1 | 2.8 | 4.8 | . 6653 12543 | 145 | 30.1 | 2.8 | 4.8 | . 6695 4000 | 150 | 30.1 | 2.8 | 4.8 | . 6699 6836 | 145 | 30.1 | 2.8 | 4.8 | . 6651 200 | 150 | 30.1 | 2.8 | 4.8 | . 6675 15200 | 145 | 30.1 | 2.8 | 4.8 | . 6659 16634 | 145 | 30.1 | 2.8 | 4.8 | . 6672 3104 | 145 | 30.1 | 2.8 | 4.8 | . 6663 8967 | 150 | 30.1 | 2.8 | 4.8 | . 6666 11404 | 145 | 30.1 | 2.8 | 4.8 | . 6661 3390 | 145 | 30.1 | 2.8 | 4.8 | . 6670 1000 | 145 | 30.1 | 2.8 | 4.8 | . 6671 8813 | 145 | 30.1 | 2.8 | 4.8 | . 799 45229 | 330 | 30.4 | 2.0 | 4.0 | . 6409 44000 | 325 | 30.7 | 2.0 | 4.0 | . 6690 80750 | 325 | 31.0 | 3.0 | 5.0 | . 6696 113000 | 555 | 31.4 | 3.0 | 5.0 | . 761 61000 | 330 | 31.4 | 2.0 | 4.0 | . 6691 160000 | 325 | 31.4 | 3.0 | 5.0 | . 6686 174419 | 565 | 31.4 | 3.0 | 5.0 | . 785 116000 | 330 | 31.4 | 2.0 | 4.0 | . 776 140000 | 325 | 32.1 | 2.0 | 4.0 | . 763 113000 | 325 | 32.1 | 2.0 | 4.0 | . 6556 30848 | 260 | 32.8 | 3.0 | 5.0 | . 70 4000 | 145 | 32.8 | 2.0 | 4.0 | . 6629 92007 | 260 | 32.8 | 3.0 | 5.0 | . data[&quot;mpg_weighted&quot;] = data[&#39;mpg&#39;] - data.mpg.min() . data.sort_values(&#39;mpg&#39;).head(50) . model year price transmission mileage fuelType tax mpg engineSize engineSize2 mpg_weighted . 6562 Hilux | 2020 | 18495 | Manual | 50 | Diesel | 260 | 2.8 | 2.4 | 4.4 | 0.0 | . 6552 Hilux | 2019 | 28495 | Automatic | 3350 | Diesel | 260 | 2.8 | 2.4 | 4.4 | 0.0 | . 6594 Hilux | 2020 | 39257 | Automatic | 1259 | Diesel | 265 | 2.8 | 2.4 | 4.4 | 0.0 | . 6595 Hilux | 2019 | 27850 | Automatic | 10250 | Diesel | 260 | 2.8 | 2.4 | 4.4 | 0.0 | . 6617 Hilux | 2020 | 36995 | Automatic | 150 | Diesel | 260 | 2.8 | 2.4 | 4.4 | 0.0 | . 6575 Hilux | 2019 | 26500 | Automatic | 7123 | Diesel | 260 | 2.8 | 2.4 | 4.4 | 0.0 | . 6596 Hilux | 2020 | 30500 | Automatic | 5000 | Diesel | 260 | 2.8 | 2.4 | 4.4 | 0.0 | . 6563 Hilux | 2020 | 23495 | Manual | 100 | Diesel | 260 | 2.8 | 2.4 | 4.4 | 0.0 | . 6583 Hilux | 2019 | 26995 | Automatic | 4 | Diesel | 265 | 2.8 | 2.4 | 4.4 | 0.0 | . 6576 Hilux | 2019 | 20500 | Manual | 5190 | Diesel | 260 | 2.8 | 2.4 | 4.4 | 0.0 | . 5966 C-HR | 2018 | 16690 | Manual | 16429 | Petrol | 145 | 6.0 | 1.2 | 3.2 | 3.2 | . 6694 Land Cruiser | 1998 | 19990 | Manual | 100000 | Diesel | 265 | 23.9 | 4.2 | 6.2 | 21.1 | . 6693 Land Cruiser | 2004 | 6450 | Automatic | 143000 | Diesel | 325 | 27.2 | 3.0 | 5.0 | 24.4 | . 6698 Land Cruiser | 2014 | 44990 | Automatic | 60000 | Diesel | 540 | 29.7 | 4.5 | 6.5 | 26.9 | . 6667 Land Cruiser | 2019 | 50995 | Semi-Auto | 6254 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6680 Land Cruiser | 2019 | 48995 | Automatic | 789 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6677 Land Cruiser | 2019 | 42990 | Semi-Auto | 22845 | Diesel | 150 | 30.1 | 2.8 | 4.8 | 27.3 | . 6649 Land Cruiser | 2019 | 39498 | Semi-Auto | 11712 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6657 Land Cruiser | 2019 | 42444 | Semi-Auto | 10083 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6658 Land Cruiser | 2019 | 40999 | Semi-Auto | 11619 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6682 Land Cruiser | 2020 | 50995 | Automatic | 3500 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6684 Land Cruiser | 2020 | 47885 | Automatic | 4 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6687 Land Cruiser | 2020 | 45950 | Automatic | 27 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6688 Land Cruiser | 2020 | 52990 | Automatic | 1244 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6689 Land Cruiser | 2019 | 44995 | Automatic | 4512 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6653 Land Cruiser | 2019 | 39498 | Semi-Auto | 12543 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6695 Land Cruiser | 2020 | 54550 | Automatic | 4000 | Diesel | 150 | 30.1 | 2.8 | 4.8 | 27.3 | . 6699 Land Cruiser | 2019 | 49995 | Automatic | 6836 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6651 Land Cruiser | 2020 | 44995 | Semi-Auto | 200 | Diesel | 150 | 30.1 | 2.8 | 4.8 | 27.3 | . 6675 Land Cruiser | 2019 | 44935 | Semi-Auto | 15200 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6659 Land Cruiser | 2019 | 42995 | Semi-Auto | 16634 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6672 Land Cruiser | 2020 | 52291 | Semi-Auto | 3104 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6663 Land Cruiser | 2019 | 45995 | Semi-Auto | 8967 | Diesel | 150 | 30.1 | 2.8 | 4.8 | 27.3 | . 6666 Land Cruiser | 2019 | 40995 | Semi-Auto | 11404 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6661 Land Cruiser | 2020 | 50995 | Semi-Auto | 3390 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6670 Land Cruiser | 2019 | 54991 | Semi-Auto | 1000 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 6671 Land Cruiser | 2019 | 47795 | Semi-Auto | 8813 | Diesel | 145 | 30.1 | 2.8 | 4.8 | 27.3 | . 799 RAV4 | 2004 | 5495 | Automatic | 45229 | Petrol | 330 | 30.4 | 2.0 | 4.0 | 27.6 | . 6409 Avensis | 2004 | 3495 | Automatic | 44000 | Petrol | 325 | 30.7 | 2.0 | 4.0 | 27.9 | . 6690 Land Cruiser | 2005 | 8000 | Manual | 80750 | Diesel | 325 | 31.0 | 3.0 | 5.0 | 28.2 | . 6696 Land Cruiser | 2006 | 7240 | Automatic | 113000 | Diesel | 555 | 31.4 | 3.0 | 5.0 | 28.6 | . 761 RAV4 | 2008 | 5195 | Automatic | 61000 | Petrol | 330 | 31.4 | 2.0 | 4.0 | 28.6 | . 6691 Land Cruiser | 2004 | 5975 | Automatic | 160000 | Diesel | 325 | 31.4 | 3.0 | 5.0 | 28.6 | . 6686 Land Cruiser | 2008 | 6950 | Automatic | 174419 | Diesel | 565 | 31.4 | 3.0 | 5.0 | 28.6 | . 785 RAV4 | 2008 | 4480 | Automatic | 116000 | Petrol | 330 | 31.4 | 2.0 | 4.0 | 28.6 | . 776 RAV4 | 2002 | 1600 | Manual | 140000 | Petrol | 325 | 32.1 | 2.0 | 4.0 | 29.3 | . 763 RAV4 | 2005 | 2394 | Manual | 113000 | Petrol | 325 | 32.1 | 2.0 | 4.0 | 29.3 | . 6556 Hilux | 2016 | 19498 | Automatic | 30848 | Diesel | 260 | 32.8 | 3.0 | 5.0 | 30.0 | . 70 GT86 | 2019 | 24990 | Automatic | 4000 | Petrol | 145 | 32.8 | 2.0 | 4.0 | 30.0 | . 6629 Hilux | 2013 | 12000 | Automatic | 92007 | Diesel | 260 | 32.8 | 3.0 | 5.0 | 30.0 | . num = [&#39;mileage&#39;, &#39;tax&#39;, &#39;mpg&#39;, &#39;engineSize&#39;] # num = [&#39;mileage&#39;, &#39;tax&#39;, &#39;mpg&#39;, &#39;engineSize&#39;, &#39;mpg_weighted&#39;] . num = [&#39;mileage&#39;, &#39;tax&#39;, &#39;mpg&#39;, &#39;engineSize&#39;] for i in range(0,len(num),1): x = pd.DataFrame(data[num[0:i+1]]) y = data[&#39;price&#39;] model = linear_model.LinearRegression() model.fit(x, y) model.coef_ . array([-1.47097078e-01, 2.71207473e+00, 3.22056993e+01, 1.18159759e+04]) . for i in range(0,len(num),1): x = pd.DataFrame(data[num[0:i+1]]) y = data[&#39;price&#39;] model = linear_model.LinearRegression() model.fit(x, y, sample_weight = data[&#39;mpg_weighted&#39;]) model.coef_ . array([-1.34164121e-01, 5.10535692e+00, 4.22850836e+01, 1.15948701e+04]) .",
            "url": "https://mdsohelmahmood.github.io/data-science/2022/04/21/Simple-Stepwise-and-Weighted-Regression.html",
            "relUrl": "/2022/04/21/Simple-Stepwise-and-Weighted-Regression.html",
            "date": " • Apr 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Simple Explanation of Statsmodels Summary",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt . import statsmodels.api as sm import statsmodels.formula.api as smf import pylab as py . df = pd.read_csv(&quot;salary.csv&quot;) df . YearsExperience Projects People_managing Salary . 0 1.1 | 4 | 0 | 39343 | . 1 1.3 | 5 | 0 | 46205 | . 2 1.5 | 6 | 0 | 37731 | . 3 2.0 | 3 | 1 | 43525 | . 4 2.2 | 5 | 1 | 39891 | . 5 2.9 | 6 | 1 | 56642 | . 6 3.0 | 8 | 1 | 60150 | . 7 3.2 | 7 | 1 | 54445 | . 8 3.2 | 9 | 2 | 64445 | . 9 3.7 | 10 | 2 | 57189 | . 10 3.9 | 15 | 2 | 63218 | . 11 4.0 | 12 | 2 | 55794 | . 12 4.0 | 7 | 2 | 56957 | . 13 4.1 | 22 | 3 | 57081 | . 14 4.5 | 12 | 3 | 61111 | . 15 4.9 | 20 | 3 | 67938 | . 16 5.1 | 21 | 3 | 66029 | . 17 5.3 | 16 | 3 | 83088 | . 18 5.9 | 28 | 4 | 81363 | . 19 6.0 | 23 | 4 | 93940 | . 20 6.8 | 25 | 4 | 91738 | . 21 7.1 | 22 | 4 | 98273 | . 22 7.9 | 35 | 4 | 101302 | . 23 8.2 | 31 | 4 | 113812 | . 24 8.7 | 27 | 4 | 109431 | . 25 9.0 | 24 | 5 | 105582 | . 26 9.5 | 33 | 5 | 116969 | . 27 9.6 | 30 | 5 | 112635 | . 28 10.3 | 29 | 5 | 122391 | . 29 10.5 | 31 | 5 | 121872 | . 30 11.0 | 28 | 5 | 126522 | . fig = plt.figure(figsize=(15,10)) plt.plot(df[&#39;People_managing&#39;], df[&#39;Salary&#39;], &#39;o&#39;) plt.grid() . fig = plt.figure(figsize=(15,10)) plt.plot(df[&#39;Projects&#39;], df[&#39;Salary&#39;], &#39;o&#39;) plt.grid() . fig = plt.figure(figsize=(15,10)) plt.plot(df[&#39;YearsExperience&#39;], df[&#39;Salary&#39;], &#39;o&#39;) plt.xlabel(&#39;YearsExperience&#39;) plt.ylabel(&#39;Salary&#39;) plt.grid() . model = smf.ols(formula = &#39;Salary ~ Projects + People_managing + YearsExperience&#39;, data = df) # model = smf.ols(formula = &#39;Salary ~ YearsExperience&#39;, data = df) # model = smf.ols(formula = &#39;Salary ~ Projects&#39;, data = df) model = model.fit() # model = model.fit(cov_type=&quot;hc0&quot;) . # model.predict(y) . print(model.summary()) . OLS Regression Results ============================================================================== Dep. Variable: Salary R-squared: 0.963 Model: OLS Adj. R-squared: 0.959 Method: Least Squares F-statistic: 235.6 Date: Thu, 21 Apr 2022 Prob (F-statistic): 1.82e-19 Time: 18:23:38 Log-Likelihood: -310.21 No. Observations: 31 AIC: 628.4 Df Residuals: 27 BIC: 634.2 Df Model: 3 Covariance Type: nonrobust =================================================================================== coef std err t P&gt;|t| [0.025 0.975] -- Intercept 2.567e+04 2221.384 11.556 0.000 2.11e+04 3.02e+04 Projects 333.4580 282.859 1.179 0.249 -246.920 913.836 People_managing -2447.4776 2426.626 -1.009 0.322 -7426.503 2531.548 YearsExperience 9633.2604 1210.014 7.961 0.000 7150.516 1.21e+04 ============================================================================== Omnibus: 2.060 Durbin-Watson: 1.958 Prob(Omnibus): 0.357 Jarque-Bera (JB): 1.859 Skew: 0.555 Prob(JB): 0.395 Kurtosis: 2.545 Cond. No. 56.0 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. .",
            "url": "https://mdsohelmahmood.github.io/data-science/2022/04/21/Simple-Explanation-of-Statsmodels-Summary.html",
            "relUrl": "/2022/04/21/Simple-Explanation-of-Statsmodels-Summary.html",
            "date": " • Apr 21, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Practical implementation of outlier detection in python",
            "content": "import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np from matplotlib.pyplot import figure . df=pd.read_csv(&#39;listings.csv&#39;) . df.head() . id listing_url scrape_id last_scraped name summary space description experiences_offered neighborhood_overview ... review_scores_value requires_license license jurisdiction_names instant_bookable cancellation_policy require_guest_profile_picture require_guest_phone_verification calculated_host_listings_count reviews_per_month . 0 12147973 | https://www.airbnb.com/rooms/12147973 | 2.016090e+13 | 9/7/2016 | Sunny Bungalow in the City | Cozy, sunny, family home. Master bedroom high... | The house has an open and cozy feel at the sam... | Cozy, sunny, family home. Master bedroom high... | none | Roslindale is quiet, convenient and friendly. ... | ... | NaN | f | NaN | NaN | f | moderate | f | f | 1 | NaN | . 1 3075044 | https://www.airbnb.com/rooms/3075044 | 2.016090e+13 | 9/7/2016 | Charming room in pet friendly apt | Charming and quiet room in a second floor 1910... | Small but cozy and quite room with a full size... | Charming and quiet room in a second floor 1910... | none | The room is in Roslindale, a diverse and prima... | ... | 9.0 | f | NaN | NaN | t | moderate | f | f | 1 | 1.30 | . 2 6976 | https://www.airbnb.com/rooms/6976 | 2.016090e+13 | 9/7/2016 | Mexican Folk Art Haven in Boston | Come stay with a friendly, middle-aged guy in ... | Come stay with a friendly, middle-aged guy in ... | Come stay with a friendly, middle-aged guy in ... | none | The LOCATION: Roslindale is a safe and diverse... | ... | 10.0 | f | NaN | NaN | f | moderate | t | f | 1 | 0.47 | . 3 1436513 | https://www.airbnb.com/rooms/1436513 | 2.016090e+13 | 9/7/2016 | Spacious Sunny Bedroom Suite in Historic Home | Come experience the comforts of home away from... | Most places you find in Boston are small howev... | Come experience the comforts of home away from... | none | Roslindale is a lovely little neighborhood loc... | ... | 10.0 | f | NaN | NaN | f | moderate | f | f | 1 | 1.00 | . 4 7651065 | https://www.airbnb.com/rooms/7651065 | 2.016090e+13 | 9/7/2016 | Come Home to Boston | My comfy, clean and relaxing home is one block... | Clean, attractive, private room, one block fro... | My comfy, clean and relaxing home is one block... | none | I love the proximity to downtown, the neighbor... | ... | 10.0 | f | NaN | NaN | f | flexible | f | f | 1 | 2.25 | . 5 rows × 95 columns . def remove_sign(x,sign): if type(x) is str: x = float(x.replace(sign,&#39;&#39;).replace(&#39;,&#39;,&#39;&#39;)) return x . df[&#39;price&#39;] . 0 $250.00 1 $65.00 2 $65.00 3 $75.00 4 $79.00 ... 3580 $69.00 3581 $150.00 3582 $198.00 3583 $65.00 3584 $65.00 Name: price, Length: 3585, dtype: object . df=df[[&#39;price&#39;,&#39;property_type&#39;]] df=pd.DataFrame(df) figure(figsize=(12, 8), dpi=80) df.price = df.price.apply(remove_sign,sign=&#39;$&#39;) sns.boxplot(y=&#39;price&#39;, x=&#39;property_type&#39;,data=df) plt.xticks(rotation=90) plt.ylabel(&#39;Price ($)&#39;) . Text(0, 0.5, &#39;Price ($)&#39;) . def remove_outlier_IQR(df): Q1=df.quantile(0.25) Q3=df.quantile(0.75) IQR=Q3-Q1 df_final=df[~((df&lt;(Q1-1.5*IQR)) | (df&gt;(Q3+1.5*IQR)))] return df_final . df_outlier_removed=remove_outlier_IQR(df.price) df_outlier_removed=pd.DataFrame(df_outlier_removed) ind_diff=df.index.difference(df_outlier_removed.index) figure(figsize=(12, 8), dpi=80) for i in range(0, len(ind_diff),1): df_final=df.drop([ind_diff[i]]) df=df_final sns.boxplot(y=&#39;price&#39;, x=&#39;property_type&#39;,data=df_final) plt.xticks(rotation=90) plt.ylabel(&#39;Price ($)&#39;) . Text(0, 0.5, &#39;Price ($)&#39;) . def remove_outlier_Hampel(df): med=df.median() List=abs(df-med) cond=List.median()*4.5 good_list=List[~(List&gt;cond)] return good_list . df=pd.read_csv(&#39;listings.csv&#39;) . df.price = df.price.apply(remove_sign,sign=&#39;$&#39;) . df_outlier_removed=remove_outlier_Hampel(df.price) df_outlier_removed=pd.DataFrame(df_outlier_removed) ind_diff=df.index.difference(df_outlier_removed.index) for i in range(0, len(ind_diff),1): df_final=df.drop([ind_diff[i]]) df=df_final sns.boxplot(y=&#39;price&#39;, x=&#39;property_type&#39;,data=df_final) plt.xticks(rotation=90) plt.ylabel(&#39;Price ($)&#39;) . Text(0, 0.5, &#39;Price ($)&#39;) . len(ind_diff) . 95 . from sklearn.cluster import DBSCAN from sklearn.neighbors import NearestNeighbors . def remove_outliers_DBSCAN(df,eps,min_samples): outlier_detection = DBSCAN(eps = eps, min_samples = min_samples) clusters = outlier_detection.fit_predict(df.values.reshape(-1,1)) data = pd.DataFrame() data[&#39;cluster&#39;] = clusters return data[&#39;cluster&#39;] . df=pd.read_csv(&#39;listings.csv&#39;) df.price = df.price.apply(remove_sign,sign=&#39;$&#39;) . clusters=remove_outliers_DBSCAN((df[&#39;price&#39;]),0.5,5) clusters.value_counts().sort_values(ascending=False) . -1 384 9 144 21 117 4 101 0 95 ... 81 6 56 5 82 5 124 5 8 5 Name: cluster, Length: 127, dtype: int64 . plt.plot(clusters) . [&lt;matplotlib.lines.Line2D at 0x282abed7748&gt;] . df_cluster=pd.DataFrame(clusters) ind_outlier=df_cluster.index[df_cluster[&#39;cluster&#39;]==-1] ind_outlier . Int64Index([ 12, 40, 70, 75, 81, 84, 96, 99, 100, 107, ... 3501, 3529, 3532, 3539, 3550, 3552, 3565, 3572, 3576, 3582], dtype=&#39;int64&#39;, length=384) . for i in range(0, len(ind_outlier),1): df_final=df.drop([ind_outlier[i]]) df=df_final sns.boxplot(y=&#39;price&#39;, x=&#39;property_type&#39;,data=df_final) plt.xticks(rotation=90) plt.ylabel(&#39;Price ($)&#39;) . Text(0, 0.5, &#39;Price ($)&#39;) . len(ind_outlier) . 384 . neigh = NearestNeighbors(n_neighbors=3) a=df.price.values.reshape(-1,1) nbrs = neigh.fit(a) distances, indices = nbrs.kneighbors(a) distances = np.sort(distances, axis=0) distances = distances[:,1] plt.plot(distances) . [&lt;matplotlib.lines.Line2D at 0x28297a2c548&gt;] . iris=pd.read_csv(&quot;Iris.csv&quot;) . df=iris[iris[&#39;Species&#39;]==&#39;Iris-virginica&#39;] x=df[&#39;SepalLengthCm&#39;] y=df[&#39;SepalWidthCm&#39;] plt.scatter(x,y) . &lt;matplotlib.collections.PathCollection at 0x282af0ba9c8&gt; . coef = np.polyfit(x,y,1) poly1d_fn = np.poly1d(coef) plt.plot(x,y, &#39;yo&#39;, x, poly1d_fn(x), &#39;--k&#39;) . [&lt;matplotlib.lines.Line2D at 0x282af057f48&gt;, &lt;matplotlib.lines.Line2D at 0x282af02bec8&gt;] . x=x.append(pd.Series([20])) y=y.append(pd.Series([6.08])) . coef . array([0.23161465, 1.44811456]) . 20*0.23189+1.446 . 6.0838 . coef = np.polyfit(x,y,1) poly1d_fn = np.poly1d(coef) plt.plot(x,y, &#39;yo&#39;, x, poly1d_fn(x), &#39;--k&#39;) . [&lt;matplotlib.lines.Line2D at 0x282af0f6bc8&gt;, &lt;matplotlib.lines.Line2D at 0x282af165d08&gt;] .",
            "url": "https://mdsohelmahmood.github.io/data-science/2022/04/19/Practical-implementation-of-outlier-detection-in-python.html",
            "relUrl": "/2022/04/19/Practical-implementation-of-outlier-detection-in-python.html",
            "date": " • Apr 19, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "T-distribution",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import random from scipy.stats import norm . df = pd.read_excel(&#39;data.xlsx&#39;) . fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(df.data, bins=30, color= &#39;green&#39;) . (array([ 3., 4., 24., 144., 594., 1387., 2024., 2040., 1520., 1220., 1116., 1099., 1193., 1212., 1214., 1341., 1422., 1770., 1927., 1995., 1991., 1709., 1309., 851., 484., 263., 102., 30., 8., 4.]), array([29.77 , 32.076, 34.382, 36.688, 38.994, 41.3 , 43.606, 45.912, 48.218, 50.524, 52.83 , 55.136, 57.442, 59.748, 62.054, 64.36 , 66.666, 68.972, 71.278, 73.584, 75.89 , 78.196, 80.502, 82.808, 85.114, 87.42 , 89.726, 92.032, 94.338, 96.644, 98.95 ]), &lt;BarContainer object of 30 artists&gt;) . df.data.mean() . 63.30878300000027 . d = [] sample_number = 50 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = np.mean(b) d.append(m) print(np.mean(d)) fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d, bins=30, color= &#39;green&#39;) . 63.306354999999996 . (array([ 1., 4., 5., 8., 14., 13., 30., 30., 38., 58., 53., 51., 84., 78., 83., 68., 83., 51., 62., 41., 35., 43., 25., 9., 10., 6., 10., 3., 3., 1.]), array([57.778 , 58.16018667, 58.54237333, 58.92456 , 59.30674667, 59.68893333, 60.07112 , 60.45330667, 60.83549333, 61.21768 , 61.59986667, 61.98205333, 62.36424 , 62.74642667, 63.12861333, 63.5108 , 63.89298667, 64.27517333, 64.65736 , 65.03954667, 65.42173333, 65.80392 , 66.18610667, 66.56829333, 66.95048 , 67.33266667, 67.71485333, 68.09704 , 68.47922667, 68.86141333, 69.2436 ]), &lt;BarContainer object of 30 artists&gt;) . d = [] sample_number = 50 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = np.mean(b) d.append(m) print(np.mean(d)) fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d, bins=30, color= &#39;red&#39;) d = [] sample_number = 500 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = np.mean(b) d.append(m) print(np.mean(d)) # fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d, bins=30, color= &#39;green&#39;) . 63.33670059999999 63.32107628 . (array([ 3., 1., 3., 8., 13., 13., 21., 33., 21., 41., 67., 64., 97., 91., 83., 97., 77., 67., 55., 39., 31., 23., 17., 13., 9., 4., 2., 1., 4., 2.]), array([61.45406 , 61.58445333, 61.71484667, 61.84524 , 61.97563333, 62.10602667, 62.23642 , 62.36681333, 62.49720667, 62.6276 , 62.75799333, 62.88838667, 63.01878 , 63.14917333, 63.27956667, 63.40996 , 63.54035333, 63.67074667, 63.80114 , 63.93153333, 64.06192667, 64.19232 , 64.32271333, 64.45310667, 64.5835 , 64.71389333, 64.84428667, 64.97468 , 65.10507333, 65.23546667, 65.36586 ]), &lt;BarContainer object of 30 artists&gt;) . def t_stat(x): n = len(x) m = 63.3 s = np.std(x) t = (np.mean(x) - m)/(s/np.sqrt(n)) return t . d = [] sample_number = 5 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = t_stat(b) d.append(m) fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d, bins=30, color= &#39;yellow&#39;) plt.xlabel(&quot;t values&quot;) plt.ylabel(&quot;f(t)&quot;) . Text(0, 0.5, &#39;f(t)&#39;) . d = [] sample_number = 10 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = t_stat(b) d.append(m) fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d, bins=30, color= &#39;blue&#39;) plt.xlabel(&quot;t values&quot;) plt.ylabel(&quot;f(t)&quot;) . Text(0, 0.5, &#39;f(t)&#39;) . d = [] sample_number = 50 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = t_stat(b) d.append(m) fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d, bins=30, color= &#39;green&#39;) plt.xlabel(&quot;t values&quot;) plt.ylabel(&quot;f(t)&quot;) . Text(0, 0.5, &#39;f(t)&#39;) . d = [] sample_number = 5 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = t_stat(b) d.append(m) fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d, bins=30, color= &#39;yellow&#39;) plt.xlabel(&quot;t values&quot;) plt.ylabel(&quot;f(t)&quot;) d = [] sample_number = 10 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = t_stat(b) d.append(m) # fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d, bins=30, color= &#39;blue&#39;) plt.xlabel(&quot;t values&quot;) plt.ylabel(&quot;f(t)&quot;) d = [] sample_number = 50 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = t_stat(b) d.append(m) # fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d, bins=30, color= &#39;green&#39;) plt.xlabel(&quot;t values&quot;) plt.ylabel(&quot;f(t)&quot;) . Text(0, 0.5, &#39;f(t)&#39;) . d1 = [] sample_number = 5 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = t_stat(b) d1.append(m) fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d1, bins=30, color= &#39;yellow&#39;) plt.xlabel(&quot;t values&quot;) plt.ylabel(&quot;f(t)&quot;) d2 = [] sample_number = 10 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = t_stat(b) d2.append(m) # fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d2, bins=30, color= &#39;green&#39;) plt.xlabel(&quot;t values&quot;) plt.ylabel(&quot;f(t)&quot;) d3 = [] sample_number = 50 for i in range(1000): a = random.sample(range(0, 29999), sample_number) b = list(df.data.iloc[a]) m = t_stat(b) d3.append(m) # fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(d3, bins=30, color= &#39;blue&#39;) plt.xlabel(&quot;t values&quot;) plt.ylabel(&quot;f(t)&quot;) . Text(0, 0.5, &#39;f(t)&#39;) . fig, ax = plt.subplots(figsize=(10, 8)) sns.kdeplot(d1, label = &quot;sample size = 5&quot;) sns.kdeplot(d2, label = &quot;sample size = 10&quot;) sns.kdeplot(d3, label = &quot;sample size = 50&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x1408443dfd0&gt; .",
            "url": "https://mdsohelmahmood.github.io/data-science/2021/12/15/T-distribution.html",
            "relUrl": "/2021/12/15/T-distribution.html",
            "date": " • Dec 15, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Q-Q plot",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt . df = pd.read_excel(&#39;Normal data.xlsx&#39;, sheet_name = &#39;Normal&#39;) # df = pd.read_excel(&#39;Normal data.xlsx&#39;, sheet_name = &#39;Skewed&#39;) # df = pd.read_excel(&#39;Normal data.xlsx&#39;, sheet_name = &#39;Bimodal&#39;) # df . fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(sorted(df.data), bins=40) . (array([ 1., 4., 1., 3., 6., 14., 5., 9., 16., 20., 26., 28., 39., 44., 61., 68., 58., 75., 91., 89., 63., 57., 74., 68., 73., 64., 59., 44., 36., 22., 22., 10., 15., 7., 7., 10., 3., 3., 3., 2.]), array([20.16 , 20.4015, 20.643 , 20.8845, 21.126 , 21.3675, 21.609 , 21.8505, 22.092 , 22.3335, 22.575 , 22.8165, 23.058 , 23.2995, 23.541 , 23.7825, 24.024 , 24.2655, 24.507 , 24.7485, 24.99 , 25.2315, 25.473 , 25.7145, 25.956 , 26.1975, 26.439 , 26.6805, 26.922 , 27.1635, 27.405 , 27.6465, 27.888 , 28.1295, 28.371 , 28.6125, 28.854 , 29.0955, 29.337 , 29.5785, 29.82 ]), &lt;BarContainer object of 40 artists&gt;) . m = df.data.mean() st = df.data.std() # Standardize the data for i in range(0,df.shape[0],1): df.data.iloc[i] = (df.data.iloc[i]-m)/st . dfn = pd.read_excel(&#39;Normal data.xlsx&#39;, sheet_name = &#39;Standard&#39;) q = [] j=0 for i in range(1,dfn.shape[0]+1,1): j=i/df.shape[0] q_temp = np.quantile(dfn[&#39;data&#39;], j) q.append(q_temp) . fig, ax = plt.subplots(figsize=(10, 8)) plt.hist(sorted(dfn.data), bins=40) . (array([ 2., 5., 3., 6., 6., 7., 10., 9., 24., 24., 39., 37., 54., 52., 51., 54., 67., 89., 69., 82., 73., 51., 64., 63., 79., 50., 36., 48., 29., 24., 26., 14., 12., 11., 10., 5., 7., 3., 2., 3.]), array([-2.85709987, -2.71208322, -2.56706657, -2.42204992, -2.27703326, -2.13201661, -1.98699996, -1.84198331, -1.69696666, -1.55195001, -1.40693336, -1.26191671, -1.11690006, -0.97188341, -0.82686676, -0.68185011, -0.53683346, -0.3918168 , -0.24680015, -0.1017835 , 0.04323315, 0.1882498 , 0.33326645, 0.4782831 , 0.62329975, 0.7683164 , 0.91333305, 1.0583497 , 1.20336635, 1.348383 , 1.49339966, 1.63841631, 1.78343296, 1.92844961, 2.07346626, 2.21848291, 2.36349956, 2.50851621, 2.65353286, 2.79854951, 2.94356616]), &lt;BarContainer object of 40 artists&gt;) . fig, ax = plt.subplots(figsize=(10, 8)) plt.plot(q,sorted(df.data),&#39;o&#39;) plt.xlabel(&quot;Quantile of standard normal distribution&quot;) plt.ylabel(&quot;Sample Z-score&quot;) plt.grid() . import statsmodels.api as sm . sm.qqplot(df.data, line =&#39;45&#39;) .",
            "url": "https://mdsohelmahmood.github.io/data-science/2021/11/28/QQplot.html",
            "relUrl": "/2021/11/28/QQplot.html",
            "date": " • Nov 28, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Generalized Additive Models",
            "content": "import numpy as np import pandas as pd import statsmodels.api as sm import statsmodels.formula.api as smf from sklearn.metrics import r2_score from pygam import LinearGAM, s, f, l, te from pygam.datasets import wage . def partialResidualPlot(model, df, outcome, feature, ax): y_pred = model.predict(df) copy_df = df.copy() for c in copy_df.columns: if c == feature: continue copy_df[c] = 0.0 feature_prediction = model.predict(copy_df) results = pd.DataFrame({ &#39;feature&#39;: df[feature], &#39;residual&#39;: df[outcome] - y_pred, &#39;ypartial&#39;: feature_prediction - model.params[0], }) results = results.sort_values(by=[&#39;feature&#39;]) smoothed = sm.nonparametric.lowess(results.ypartial, results.feature, frac=1/3) ax.scatter(results.feature, results.ypartial + results.residual) # ax.plot(smoothed[:, 0], smoothed[:, 1], color=&#39;gray&#39;) ax.plot(results.feature, results.ypartial, color=&#39;black&#39;) ax.set_xlabel(feature) ax.set_ylabel(f&#39;Residual + {feature} contribution&#39;) return ax . data = pd.read_csv(&quot;Fish.csv&quot;) data . Species Length1 Length2 Length3 Height Width Weight . 0 Bream | 23.2 | 25.4 | 30.0 | 11.5200 | 4.0200 | 242.0 | . 1 Bream | 24.0 | 26.3 | 31.2 | 12.4800 | 4.3056 | 290.0 | . 2 Bream | 23.9 | 26.5 | 31.1 | 12.3778 | 4.6961 | 340.0 | . 3 Bream | 26.3 | 29.0 | 33.5 | 12.7300 | 4.4555 | 363.0 | . 4 Bream | 26.5 | 29.0 | 34.0 | 12.4440 | 5.1340 | 430.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 154 Smelt | 11.5 | 12.2 | 13.4 | 2.0904 | 1.3936 | 12.2 | . 155 Smelt | 11.7 | 12.4 | 13.5 | 2.4300 | 1.2690 | 13.4 | . 156 Smelt | 12.1 | 13.0 | 13.8 | 2.2770 | 1.2558 | 12.2 | . 157 Smelt | 13.2 | 14.3 | 15.2 | 2.8728 | 2.0672 | 19.7 | . 158 Smelt | 13.8 | 15.0 | 16.2 | 2.9322 | 1.8792 | 19.9 | . 159 rows × 7 columns . fig, ax = plt.subplots(figsize=(10, 8)) y = data.Weight x = data.Width plt.scatter(x,y) plt.ylabel(&quot;weight of fish in gram&quot;) plt.xlabel(&quot;diagonal width in cm&quot;) plt.grid() . Polynomial . result_poly = smf.ols(&#39;Weight ~ Width +&#39; + &#39;I(Width**2)&#39;, data=data).fit() print(result_poly.summary()) . OLS Regression Results ============================================================================== Dep. Variable: Weight R-squared: 0.827 Model: OLS Adj. R-squared: 0.825 Method: Least Squares F-statistic: 374.2 Date: Sun, 14 Nov 2021 Prob (F-statistic): 2.96e-60 Time: 23:28:18 Log-Likelihood: -1020.4 No. Observations: 159 AIC: 2047. Df Residuals: 156 BIC: 2056. Df Model: 2 Covariance Type: nonrobust ================================================================================= coef std err t P&gt;|t| [0.025 0.975] Intercept -74.5818 67.333 -1.108 0.270 -207.584 58.420 Width -1.3809 31.714 -0.044 0.965 -64.025 61.263 I(Width ** 2) 21.4434 3.496 6.133 0.000 14.537 28.349 ============================================================================== Omnibus: 113.488 Durbin-Watson: 0.538 Prob(Omnibus): 0.000 Jarque-Bera (JB): 1239.797 Skew: 2.442 Prob(JB): 6.05e-270 Kurtosis: 15.778 Cond. No. 171. ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . fig, ax = plt.subplots(figsize=(10, 8)) pred_poly = result_poly.predict(data.Width) plt.plot(data.Width, data.Weight,&#39;o&#39;) plt.xlabel(&#39;Width&#39;) plt.ylabel(&#39;Weight&#39;) pfit = pd.DataFrame(columns = [&#39;Width&#39;,&#39;Pred&#39;]) pfit.Width = data.Width pfit.Pred = pred_poly pfit = pfit.sort_values(by=[&#39;Width&#39;]) plt.plot(pfit.Width, pfit.Pred) plt.grid() . Spline . formula = (&#39;Weight ~ bs(Width, df=3, degree=2)&#39;) model_spline = smf.ols(formula=formula, data=data) result_spline = model_spline.fit() print(result_spline.summary()) . OLS Regression Results ============================================================================== Dep. Variable: Weight R-squared: 0.849 Model: OLS Adj. R-squared: 0.846 Method: Least Squares F-statistic: 289.8 Date: Sun, 14 Nov 2021 Prob (F-statistic): 2.55e-63 Time: 22:14:30 Log-Likelihood: -1010.0 No. Observations: 159 AIC: 2028. Df Residuals: 155 BIC: 2040. Df Model: 3 Covariance Type: nonrobust ================================================================================================ coef std err t P&gt;|t| [0.025 0.975] Intercept 48.2672 43.391 1.112 0.268 -37.447 133.982 bs(Width, df=3, degree=2)[0] -190.7432 67.060 -2.844 0.005 -323.212 -58.275 bs(Width, df=3, degree=2)[1] 804.1160 55.914 14.381 0.000 693.665 914.567 bs(Width, df=3, degree=2)[2] 1094.3693 79.115 13.833 0.000 938.086 1250.652 ============================================================================== Omnibus: 123.081 Durbin-Watson: 0.608 Prob(Omnibus): 0.000 Jarque-Bera (JB): 1516.730 Skew: 2.688 Prob(JB): 0.00 Kurtosis: 17.143 Cond. No. 10.6 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . fig, ax = plt.subplots(figsize=(10, 8)) partialResidualPlot(result_spline, data, &#39;Weight&#39;, &#39;Width&#39;, ax) plt.tight_layout() plt.grid() plt.xlabel(&#39;Width&#39;) plt.ylabel(&#39;Weight&#39;) . Text(53.5, 0.5, &#39;Weight&#39;) . GAM . data . Species Length1 Length2 Length3 Height Width Weight . 0 Bream | 23.2 | 25.4 | 30.0 | 11.5200 | 4.0200 | 242.0 | . 1 Bream | 24.0 | 26.3 | 31.2 | 12.4800 | 4.3056 | 290.0 | . 2 Bream | 23.9 | 26.5 | 31.1 | 12.3778 | 4.6961 | 340.0 | . 3 Bream | 26.3 | 29.0 | 33.5 | 12.7300 | 4.4555 | 363.0 | . 4 Bream | 26.5 | 29.0 | 34.0 | 12.4440 | 5.1340 | 430.0 | . ... ... | ... | ... | ... | ... | ... | ... | . 154 Smelt | 11.5 | 12.2 | 13.4 | 2.0904 | 1.3936 | 12.2 | . 155 Smelt | 11.7 | 12.4 | 13.5 | 2.4300 | 1.2690 | 13.4 | . 156 Smelt | 12.1 | 13.0 | 13.8 | 2.2770 | 1.2558 | 12.2 | . 157 Smelt | 13.2 | 14.3 | 15.2 | 2.8728 | 2.0672 | 19.7 | . 158 Smelt | 13.8 | 15.0 | 16.2 | 2.9322 | 1.8792 | 19.9 | . 159 rows × 7 columns . predictors = [&#39;Width&#39;] outcome = [&#39;Weight&#39;] x = data[predictors].values y = data[outcome] gam = LinearGAM(l(0)) gam.gridsearch(x, y) fig, ax = plt.subplots(figsize=(10, 8)) XX = gam.generate_X_grid(term=0) plt.plot(XX, gam.predict(XX), &#39;r--&#39;) plt.plot(XX, gam.prediction_intervals(XX, width=.95), color=&#39;b&#39;, ls=&#39;--&#39;) plt.scatter(X, y, facecolor=&#39;gray&#39;, edgecolors=&#39;none&#39;) . 100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time: 0:00:00 . &lt;matplotlib.collections.PathCollection at 0x151a903d5e0&gt; . predictors = [&#39;Width&#39;] outcome = [&#39;Weight&#39;] x = data[predictors].values y = data[outcome] gam = LinearGAM(s(0, n_splines=20)) gam.gridsearch(x, y) fig, ax = plt.subplots(figsize=(10, 8)) XX = gam.generate_X_grid(term=0) plt.plot(XX, gam.predict(XX), &#39;r--&#39;) plt.plot(XX, gam.prediction_intervals(XX, width=.95), color=&#39;b&#39;, ls=&#39;--&#39;) plt.scatter(X, y, facecolor=&#39;gray&#39;, edgecolors=&#39;none&#39;) . 100% (11 of 11) |########################| Elapsed Time: 0:00:00 Time: 0:00:00 . &lt;matplotlib.collections.PathCollection at 0x151a9765070&gt; .",
            "url": "https://mdsohelmahmood.github.io/data-science/2021/11/17/GAM.html",
            "relUrl": "/2021/11/17/GAM.html",
            "date": " • Nov 17, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Spline Regression",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import statsmodels.api as sm import statsmodels.formula.api as smf from sklearn.metrics import r2_score import datetime as dt . x = np.linspace(0,1,100) y = np.sin(4*np.pi*x) noise = 0.5 * np.random.normal(size=100) y = y + noise plt.scatter(x,y) . &lt;matplotlib.collections.PathCollection at 0x1dabf488070&gt; . df = pd.DataFrame(columns= [&#39;x&#39;, &#39;y&#39;]) df.x = x df.y = y df . x y . 0 0.000000 | -1.208575 | . 1 0.010101 | 0.234388 | . 2 0.020202 | 0.483987 | . 3 0.030303 | 0.506863 | . 4 0.040404 | 0.118118 | . ... ... | ... | . 95 0.959596 | -0.134463 | . 96 0.969697 | -0.331540 | . 97 0.979798 | -0.251748 | . 98 0.989899 | -0.462755 | . 99 1.000000 | 0.483190 | . 100 rows × 2 columns . formula = (&#39;y ~ bs(x, df=3, degree=1)&#39;) model_spline = smf.ols(formula=formula, data=df) result_spline = model_spline.fit() print(result_spline.summary()) . OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.299 Model: OLS Adj. R-squared: 0.277 Method: Least Squares F-statistic: 13.65 Date: Sun, 24 Oct 2021 Prob (F-statistic): 1.73e-07 Time: 23:30:14 Log-Likelihood: -105.81 No. Observations: 100 AIC: 219.6 Df Residuals: 96 BIC: 230.0 Df Model: 3 Covariance Type: nonrobust ============================================================================================ coef std err t P&gt;|t| [0.025 0.975] -- Intercept 0.8668 0.225 3.858 0.000 0.421 1.313 bs(x, df=3, degree=1)[0] -1.3675 0.326 -4.201 0.000 -2.014 -0.721 bs(x, df=3, degree=1)[1] -0.2688 0.267 -1.008 0.316 -0.798 0.261 bs(x, df=3, degree=1)[2] -1.9703 0.323 -6.092 0.000 -2.612 -1.328 ============================================================================== Omnibus: 1.646 Durbin-Watson: 0.803 Prob(Omnibus): 0.439 Jarque-Bera (JB): 1.183 Skew: -0.249 Prob(JB): 0.553 Kurtosis: 3.189 Cond. No. 7.85 ============================================================================== Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. . fig, ax = plt.subplots(figsize=(5, 5)) partialResidualPlot(result_spline, df, &#39;y&#39;, &#39;x&#39;, ax) plt.tight_layout() plt.show() . def partialResidualPlot(model, df, outcome, feature, ax): y_pred = model.predict(df) copy_df = df.copy() for c in copy_df.columns: if c == feature: continue copy_df[c] = 0.0 feature_prediction = model.predict(copy_df) results = pd.DataFrame({ &#39;feature&#39;: df[feature], &#39;residual&#39;: df[outcome] - y_pred, &#39;ypartial&#39;: feature_prediction - model.params[0], }) results = results.sort_values(by=[&#39;feature&#39;]) smoothed = sm.nonparametric.lowess(results.ypartial, results.feature, frac=1/3) ax.scatter(results.feature, results.ypartial + results.residual) # ax.plot(smoothed[:, 0], smoothed[:, 1], color=&#39;gray&#39;) ax.plot(results.feature, results.ypartial, color=&#39;black&#39;) ax.set_xlabel(feature) ax.set_ylabel(f&#39;Residual + {feature} contribution&#39;) return ax .",
            "url": "https://mdsohelmahmood.github.io/data-science/2021/10/24/Spline-Regression.html",
            "relUrl": "/2021/10/24/Spline-Regression.html",
            "date": " • Oct 24, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mdsohelmahmood.github.io/data-science/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mdsohelmahmood.github.io/data-science/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://mdsohelmahmood.github.io/data-science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mdsohelmahmood.github.io/data-science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}